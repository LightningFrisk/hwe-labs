key
value
topic
partition
offset
timestamp
timestampType


.selectExpr("split(value, '\t')[0] AS marketplace"
               ,"split(value, '\t')[1] AS customer_id"
               ,"split(value, '\t')[2] AS review_id"
               ,"split(value, '\t')[3] AS product_id"
               ,"split(value, '\t')[4] AS product_parent"
               ,"split(value, '\t')[5] AS product_title"
               ,"split(value, '\t')[6] AS product_category"
               ,"cast(split(value, '\t')[7] as int) AS star_rating"
               ,"cast(split(value, '\t')[8] as int) AS helpful_votes"
               ,"cast(split(value, '\t')[9] as int) AS total_votes"
               ,"split(value, '\t')[10] AS vine"
               ,"split(value, '\t')[11] AS verified_purchase"
               ,"split(value, '\t')[12] AS review_headline"
               ,"split(value, '\t')[13] AS review_body"
               ,"split(value, '\t')[14] AS purchase_date") \
    .withColumn("review_timestamp", current_timestamp())


# Process the received data
query = df \
    .writeStream \
    .outputMode("append") \
    .format("parquet") \
    .option("path", "s3a://hwe-fall-2023/tsagona/bronze/reviews") \
    .option("checkpointLocation", "/tmp/kafka-checkpoint") \
    .start()



from pyspark.sql import SparkSession
from pyspark.sql.functions import current_timestamp, split, col

# Initialize Spark session
spark = SparkSession.builder \
    .appName("Kafka to Parquet") \
    .getOrCreate()

# Read stream from Kafka
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_bootstrap_servers) \
    .option("subscribe", kafka_topic) \
    .option("startingOffsets", "earliest") \
    .option("maxOffsetsPerTrigger", "1000") \
    .option("kafka.security.protocol", "SASL_SSL") \
    .option("kafka.sasl.mechanism", "SCRAM-SHA-512") \
    .option("kafka.sasl.jaas.config", getScramAuthString(username, password)) \
    .load()

# Split the Kafka message on tab characters and assign field names
df = df.selectExpr("CAST(value AS STRING) as kafka_message")

# Split the message and extract fields
df = df.select(
    split(col("kafka_message"), "\t").alias("split_message")
)

df = df.select(etpla
    col("split_message").getItem(0).alias("product_title"),
    col("split_message").getItem(1).alias("star_rating"),
    col("split_message").getItem(2).alias("review_content"),
    current_timestamp().alias("review_timestamp")
)

# Write the data as Parquet files to S3
query = df.writeStream \
    .format("parquet") \
    .option("checkpointLocation", "/tmp/kafka-checkpoint") \
    .outputMode("append") \
    .option("path", "s3a://hwe-$CLASS/$HANDLE/bronze/reviews") \
    .start()

# Await termination of the stream
query.awaitTermination()
